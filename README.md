A reverse-mode autodiff (reverse automatic differentiation) computational framework that automatically builds a computational graph and performs backpropagation through the computational graph.

Also included is a model of a perceptron that can be composited to build an entire multi-layer perceptron network fully capable of gradient descent due to the reverse-mode autodiff framework.

Try training your own MLP!

Modelled around Andrej Karpathy's micrograd.
